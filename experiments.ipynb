{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Replicate experiments from:\n",
    "\"How Many Data Points is a Prompt Worth?\"\n",
    "\n",
    "This script demonstrates two fine-tuning approaches:\n",
    "  1) Head-based classification (standard).\n",
    "  2) Prompt-based classification with pattern + verbalizer.\n",
    "\n",
    "We evaluate with subsets of the training data of varying size,\n",
    "run multiple seeds, and evaluate on the dev/validation set.\n",
    "\n",
    "You should adapt the prompts/verbalizers for each dataset\n",
    "(CB, COPA, MultiRC, BoolQ, WSC, WiC, RTE, MNLI) to exactly\n",
    "match those in the paper and references therein.\n",
    "\n",
    "Author: ChatGPT\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Hugging Face libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForSequenceClassification,\n",
    "                          AutoModelForMaskedLM,\n",
    "                          DataCollatorWithPadding,\n",
    "                          Trainer, \n",
    "                          TrainingArguments)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Configuration structures\n",
    "# --------------------------\n",
    "@dataclass\n",
    "class ExperimentArguments:\n",
    "    \"\"\"\n",
    "    Configuration for the entire experiment.\n",
    "    \"\"\"\n",
    "    model_name_or_path: str = field(\n",
    "        default=\"roberta-large\", \n",
    "        metadata={\"help\": \"Pretrained model checkpoint to use (e.g., roberta-large).\"}\n",
    "    )\n",
    "    task_name: str = field(\n",
    "        default=\"boolq\", \n",
    "        metadata={\"help\": \"Which dataset to run (boolq, multirc, etc.)\"}\n",
    "    )\n",
    "    approach: str = field(\n",
    "        default=\"head\", \n",
    "        metadata={\"help\": \"Which approach to use: 'head' or 'prompt'.\"}\n",
    "    )\n",
    "    max_train_samples: int = field(\n",
    "        default=None, \n",
    "        metadata={\"help\": \"For quick testing/debugging. If set, truncate the train set.\"}\n",
    "    )\n",
    "    max_eval_samples: int = field(\n",
    "        default=None, \n",
    "        metadata={\"help\": \"For quick testing/debugging. If set, truncate the eval set.\"}\n",
    "    )\n",
    "    learning_rate: float = field(\n",
    "        default=1e-5, \n",
    "        metadata={\"help\": \"Learning rate for fine-tuning.\"}\n",
    "    )\n",
    "    train_batch_size: int = field(\n",
    "        default=8, \n",
    "        metadata={\"help\": \"Batch size for training.\"}\n",
    "    )\n",
    "    eval_batch_size: int = field(\n",
    "        default=8, \n",
    "        metadata={\"help\": \"Batch size for evaluation.\"}\n",
    "    )\n",
    "    num_train_epochs: float = field(\n",
    "        default=100.0, \n",
    "        metadata={\"help\": \"Train for as many epochs as needed (the paper uses up to 100).\"}\n",
    "    )\n",
    "    min_train_steps: int = field(\n",
    "        default=250,\n",
    "        metadata={\"help\": \"Minimum total training steps (paper uses at least 250 steps).\"}\n",
    "    )\n",
    "    # In practice, you might want more sophisticated ways to handle steps vs. epochs.\n",
    "    # Here we replicate the idea of a large # epochs but ensure we see at least 250 steps.\n",
    "    \n",
    "    output_dir: str = field(\n",
    "        default=\"./outputs\",\n",
    "        metadata={\"help\": \"Where to store checkpoints/logs/etc.\"}\n",
    "    )\n",
    "    seed: int = field(\n",
    "        default=42, \n",
    "        metadata={\"help\": \"Random seed for initialization.\"}\n",
    "    )\n",
    "    subset_sizes: List[int] = field(\n",
    "        default_factory=lambda: [10, 15, 20, 32, 50, 70, 100, 150, 200, 320, 500, 750, 1000],\n",
    "        metadata={\"help\": \"Subset sizes of training data to evaluate, in ascending order.\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    \"\"\"\n",
    "    Configuration for data loading, splitting, etc.\n",
    "    \"\"\"\n",
    "    dev_holdout_size: int = 50   # e.g. for MultiRC, CB, COPA\n",
    "    # For BoolQ, this might be 500; for RTE, WiC, WSC, etc. you must adjust accordingly.\n",
    "    # The paper sets aside specific amounts. Adjust as needed.\n",
    "\n",
    "# A dictionary to specify how many examples to hold out as dev from training,\n",
    "# for each dataset as described in the paper. Adjust as you replicate all tasks.\n",
    "DEV_SPLITS = {\n",
    "    \"boolq\": 500,\n",
    "    \"multirc\": 50,\n",
    "    \"copa\": 50,\n",
    "    \"cb\": 50,\n",
    "    \"wic\": 50,\n",
    "    \"wsc\": 50,\n",
    "    \"rte\": 50,\n",
    "    \"mnli\": 1000  # Example; you'd verify your exact scheme\n",
    "    # ... etc. for each dataset\n",
    "}\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Utility functions\n",
    "# --------------------------\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"Ensure reproducibility across libraries.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def load_superglue_dataset(task_name):\n",
    "    \"\"\"\n",
    "    Load the dataset from Hugging Face 'super_glue' or 'glue' (for MNLI) \n",
    "    or custom tasks. \n",
    "    Return the raw dataset dict with splits: train, validation, test (if available).\n",
    "    \"\"\"\n",
    "    if task_name.lower() == \"mnli\":\n",
    "        # Load the GLUE MNLI dataset\n",
    "        raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
    "    else:\n",
    "        # Load from 'super_glue'\n",
    "        raw_datasets = load_dataset(\"super_glue\", task_name.lower())\n",
    "    return raw_datasets\n",
    "\n",
    "\n",
    "# Example PVP placeholders for BoolQ.\n",
    "# You must replicate the exact patterns/verbalizers from the paper or from \n",
    "# Schick & Schütze (2020) for full fidelity.\n",
    "# This is just a single example demonstrating the structure.\n",
    "BOOLQ_PATTERNS = [\n",
    "    {\n",
    "        \"pattern\": (\n",
    "            \"{passage}. Question: {question}. Answer: <MASK>.\"\n",
    "        ),\n",
    "        \"verbalizer\": {\"Yes\": \"Yes\", \"No\": \"No\"}\n",
    "    },\n",
    "    # You could add more patterns from the paper if they used an ensemble of prompts.\n",
    "]\n",
    "\n",
    "\n",
    "def create_boolq_prompt(example):\n",
    "    \"\"\"\n",
    "    Convert a BoolQ example into a cloze-style string using the first pattern\n",
    "    from BOOLQ_PATTERNS. Return the string and the correct verbalization label.\n",
    "    \"\"\"\n",
    "    pattern_config = BOOLQ_PATTERNS[0]  # Here we pick the first pattern\n",
    "    pattern = pattern_config[\"pattern\"]\n",
    "    \n",
    "    # Insert actual text into the pattern\n",
    "    text = pattern.format(\n",
    "        passage=example[\"passage\"],\n",
    "        question=example[\"question\"]\n",
    "    )\n",
    "    # The label is \"Yes\" or \"No\", so map example[\"label\"] into the verbalizer tokens\n",
    "    label_id = example[\"label\"]  # 1 = True, 0 = False for BoolQ\n",
    "    label_str = \"Yes\" if label_id == 1 else \"No\"\n",
    "    \n",
    "    return text, label_str\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Dataset Preprocessing\n",
    "# --------------------------\n",
    "def prepare_dataset_for_head(task_name, tokenizer, raw_train, raw_dev, raw_test, max_samples=None):\n",
    "    \"\"\"\n",
    "    Standard text classification approach.\n",
    "    This function:\n",
    "      1) Prepares the input fields (e.g., 'premise', 'hypothesis', etc. for tasks).\n",
    "      2) Tokenizes.\n",
    "      3) Returns processed Dataset objects.\n",
    "    \"\"\"\n",
    "    # For simplicity, assume each dataset is either\n",
    "    #  - single-sentence classification (BoolQ, WiC, etc.), or\n",
    "    #  - pair classification (RTE, MNLI, CB, etc.).\n",
    "    # Below is an *example* for BoolQ.\n",
    "    # Extend for each dataset’s structure in a real replication.\n",
    "    \n",
    "    def preprocess_boolq(example):\n",
    "        # For head-based approach, we treat passage and question as two separate sequences\n",
    "        # for a text-classification model.\n",
    "        # Input could be: \"[CLS] passage [SEP] question [SEP]\"\n",
    "        return tokenizer(\n",
    "            example[\"passage\"],\n",
    "            example[\"question\"],\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "    \n",
    "    def preprocess_labels(example):\n",
    "        # Keep label as-is\n",
    "        example[\"labels\"] = example[\"label\"]\n",
    "        return example\n",
    "    \n",
    "    if task_name.lower() == \"boolq\":\n",
    "        # Tokenize the dataset\n",
    "        train_dataset = raw_train.map(preprocess_boolq, batched=False)\n",
    "        dev_dataset   = raw_dev.map(preprocess_boolq, batched=False)\n",
    "        test_dataset  = raw_test.map(preprocess_boolq, batched=False)\n",
    "\n",
    "        # Convert label field\n",
    "        train_dataset = train_dataset.map(preprocess_labels, batched=False)\n",
    "        dev_dataset   = dev_dataset.map(preprocess_labels, batched=False)\n",
    "        test_dataset  = test_dataset.map(preprocess_labels, batched=False)\n",
    "    else:\n",
    "        # Implement logic for other tasks as needed\n",
    "        raise NotImplementedError(f\"Head-based prep not implemented for task {task_name}\")\n",
    "    \n",
    "    if max_samples is not None:\n",
    "        train_dataset = train_dataset.select(range(min(len(train_dataset), max_samples)))\n",
    "        dev_dataset   = dev_dataset.select(range(min(len(dev_dataset), max_samples)))\n",
    "        test_dataset  = test_dataset.select(range(min(len(test_dataset), max_samples)))\n",
    "    \n",
    "    return train_dataset, dev_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_dataset_for_prompt(task_name, tokenizer, raw_train, raw_dev, raw_test, max_samples=None):\n",
    "    \"\"\"\n",
    "    Prompt-based approach: convert each example into a single string\n",
    "    containing the cloze pattern, plus we keep track of the *correct verbalization*.\n",
    "    We'll handle this by storing the entire input text as 'input_ids'\n",
    "    and the correct label token as something we can compute the cross-entropy over.\n",
    "    \"\"\"\n",
    "    # Example for BoolQ using the single PVP in BOOLQ_PATTERNS[0].\n",
    "    # For a real replication, you’d implement each pattern or an ensemble of them.\n",
    "    \n",
    "    # We treat this as a masked LM problem: the prompt includes a <mask> token,\n",
    "    # and we want the model to predict \"Yes\" or \"No\" at that mask position.\n",
    "    \n",
    "    verbalizer_map = BOOLQ_PATTERNS[0][\"verbalizer\"]  # {\"Yes\": \"Yes\", \"No\": \"No\"}\n",
    "    \n",
    "    def tokenize_boolq_prompt(example):\n",
    "        # Convert example to prompt\n",
    "        prompt_text, label_str = create_boolq_prompt(example)\n",
    "        # Insert the special <mask> token that the model expects for mask filling\n",
    "        # For RoBERTa, the mask token is \"<mask>\" by default in Hugging Face tokenizers.\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenized = tokenizer(\n",
    "            prompt_text,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # We need the ID of the correct verbalization token in the vocab.\n",
    "        # e.g., if label_str = \"Yes\", we get the ID with tokenizer.encode(\" Yes\", add_special_tokens=False)\n",
    "        # But be careful about spacing. For simplicity, let's do:\n",
    "        verbalizer_ids = tokenizer.encode(\" \" + label_str, add_special_tokens=False)\n",
    "        # The last token in that sequence is presumably the subword for the actual label (like \"Yes\", \"No\").\n",
    "        # We'll store that as \"labels\" for the masked token. \n",
    "        correct_label_id = verbalizer_ids[-1]\n",
    "        \n",
    "        # We'll also store the correct_label_id for computing the MLM loss at the <mask> position.\n",
    "        # The Trainer can compute MLM cross-entropy if we arrange the data correctly.\n",
    "        \n",
    "        # Convert to standard python ints\n",
    "        input_ids = tokenized[\"input_ids\"][0].tolist()\n",
    "        attention_mask = tokenized[\"attention_mask\"][0].tolist()\n",
    "        \n",
    "        # Identify the <mask> token position\n",
    "        mask_token_id = tokenizer.mask_token_id\n",
    "        try:\n",
    "            mask_index = input_ids.index(mask_token_id)\n",
    "        except ValueError:\n",
    "            mask_index = -1  # In case there's an error\n",
    "        \n",
    "        # Create labels: all -100 except for the <mask> position\n",
    "        labels = [-100] * len(input_ids)\n",
    "        if mask_index >= 0:\n",
    "            labels[mask_index] = correct_label_id\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "    if task_name.lower() == \"boolq\":\n",
    "        train_dataset = raw_train.map(tokenize_boolq_prompt)\n",
    "        dev_dataset   = raw_dev.map(tokenize_boolq_prompt)\n",
    "        test_dataset  = raw_test.map(tokenize_boolq_prompt)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Prompt-based prep not implemented for task {task_name}\")\n",
    "    \n",
    "    if max_samples is not None:\n",
    "        train_dataset = train_dataset.select(range(min(len(train_dataset), max_samples)))\n",
    "        dev_dataset   = dev_dataset.select(range(min(len(dev_dataset), max_samples)))\n",
    "        test_dataset  = test_dataset.select(range(min(len(test_dataset), max_samples)))\n",
    "    \n",
    "    return train_dataset, dev_dataset, test_dataset\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Training & Evaluation\n",
    "# --------------------------\n",
    "def main():\n",
    "    # 1. Parse arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--task_name\", type=str, default=\"boolq\")\n",
    "    parser.add_argument(\"--approach\", type=str, default=\"head\", help=\"'head' or 'prompt'\")\n",
    "    parser.add_argument(\"--subset_sizes\", nargs=\"+\", default=[10,15,20,32,50,70,100,150,200,320,500,750,1000], type=int)\n",
    "    parser.add_argument(\"--model_name_or_path\", type=str, default=\"roberta-large\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./outputs\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=1e-5)\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=8)\n",
    "    parser.add_argument(\"--eval_batch_size\", type=int, default=8)\n",
    "    parser.add_argument(\"--num_train_epochs\", type=float, default=100.0)\n",
    "    parser.add_argument(\"--min_train_steps\", type=int, default=250)\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    exp_args = ExperimentArguments(\n",
    "        model_name_or_path=args.model_name_or_path,\n",
    "        task_name=args.task_name,\n",
    "        approach=args.approach,\n",
    "        learning_rate=args.learning_rate,\n",
    "        train_batch_size=args.train_batch_size,\n",
    "        eval_batch_size=args.eval_batch_size,\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        min_train_steps=args.min_train_steps,\n",
    "        output_dir=args.output_dir,\n",
    "        seed=args.seed,\n",
    "        subset_sizes=args.subset_sizes\n",
    "    )\n",
    "    \n",
    "    # 2. Set seed\n",
    "    set_seed(exp_args.seed)\n",
    "    \n",
    "    # 3. Load dataset\n",
    "    raw_datasets = load_superglue_dataset(exp_args.task_name)\n",
    "    \n",
    "    # For SuperGLUE, we typically have splits: train, validation, test\n",
    "    # For some tasks (like MNLI), we have validation_mismatched, etc.\n",
    "    # The paper mentions they do not use the official test set for SuperGLUE tasks\n",
    "    # (since it's not public), but set aside part of train for dev, and use the official\n",
    "    # validation as \"test.\" We'll replicate that approach.\n",
    "    \n",
    "    # 4. Create custom dev (split from train)\n",
    "    dev_size = DEV_SPLITS.get(exp_args.task_name.lower(), 50)\n",
    "    full_train = raw_datasets[\"train\"]\n",
    "    \n",
    "    # Shuffle train set\n",
    "    full_train = full_train.shuffle(seed=exp_args.seed)\n",
    "    \n",
    "    # Reserve dev_size for dev, rest is new_train\n",
    "    dev_dataset = full_train.select(range(dev_size))\n",
    "    new_train = full_train.select(range(dev_size, len(full_train)))\n",
    "    \n",
    "    # Use the official validation as \"test\"\n",
    "    test_dataset = raw_datasets[\"validation\"]\n",
    "    \n",
    "    # 5. Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(exp_args.model_name_or_path, use_fast=True)\n",
    "    \n",
    "    # 6. For each approach, prepare the data accordingly\n",
    "    if exp_args.approach == \"head\":\n",
    "        # We'll create a classification head on top\n",
    "        # So we will use `AutoModelForSequenceClassification`\n",
    "        # and treat examples as standard classification input\n",
    "        # (e.g. input = [CLS] text [SEP] text [SEP]).\n",
    "        \n",
    "        def train_and_evaluate_head(train_subset):\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                exp_args.model_name_or_path,\n",
    "                num_labels=2  # e.g., for BoolQ yes/no\n",
    "            )\n",
    "            \n",
    "            # Preprocess\n",
    "            train_dataset_prep, dev_dataset_prep, test_dataset_prep = prepare_dataset_for_head(\n",
    "                exp_args.task_name,\n",
    "                tokenizer,\n",
    "                train_subset,\n",
    "                dev_dataset,\n",
    "                test_dataset\n",
    "            )\n",
    "            \n",
    "            data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8 if torch.cuda.is_available() else None)\n",
    "            \n",
    "            # Setup Trainer\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=os.path.join(exp_args.output_dir, f\"{exp_args.task_name}_head_{len(train_subset)}\"),\n",
    "                learning_rate=exp_args.learning_rate,\n",
    "                per_device_train_batch_size=exp_args.train_batch_size,\n",
    "                per_device_eval_batch_size=exp_args.eval_batch_size,\n",
    "                num_train_epochs=exp_args.num_train_epochs,\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                save_strategy=\"epoch\",\n",
    "                logging_dir=os.path.join(exp_args.output_dir, \"logs\"),\n",
    "                logging_steps=50,\n",
    "                seed=exp_args.seed\n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset_prep,\n",
    "                eval_dataset=dev_dataset_prep,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator\n",
    "            )\n",
    "            \n",
    "            # We replicate: train for up to N epochs, using a very low LR,\n",
    "            # ensuring at least 250 steps\n",
    "            step_per_epoch = len(train_dataset_prep) // training_args.per_device_train_batch_size\n",
    "            total_steps = step_per_epoch * training_args.num_train_epochs\n",
    "            if total_steps < exp_args.min_train_steps:\n",
    "                # we can override the num_train_epochs or switch to max_steps if needed\n",
    "                # for simplicity, let's just forcibly set max_steps = exp_args.min_train_steps\n",
    "                trainer.args.max_steps = exp_args.min_train_steps\n",
    "                trainer.args.num_train_epochs = 9999  # effectively ignore epoch-based termination\n",
    "                trainer.args.evaluation_strategy=\"steps\"\n",
    "                trainer.args.save_strategy=\"steps\"\n",
    "            \n",
    "            trainer.train()\n",
    "            # Evaluate on dev, test\n",
    "            dev_metrics = trainer.evaluate(eval_dataset=dev_dataset_prep)\n",
    "            test_metrics = trainer.evaluate(eval_dataset=test_dataset_prep)\n",
    "            \n",
    "            return dev_metrics, test_metrics\n",
    "        \n",
    "        # For each subset size\n",
    "        for size in exp_args.subset_sizes:\n",
    "            # If subset size is larger than new_train, skip\n",
    "            if size > len(new_train):\n",
    "                continue\n",
    "            train_subset = new_train.select(range(size))\n",
    "            dev_metrics, test_metrics = train_and_evaluate_head(train_subset)\n",
    "            print(f\"[HEAD] Subset={size}, Dev={dev_metrics}, Test={test_metrics}\")\n",
    "    \n",
    "    elif exp_args.approach == \"prompt\":\n",
    "        # We'll treat this as a masked language modeling problem\n",
    "        # with custom patterns and a restricted vocabulary for classification\n",
    "        # but we can implement a simple approach using `AutoModelForMaskedLM`.\n",
    "        \n",
    "        def train_and_evaluate_prompt(train_subset):\n",
    "            model = AutoModelForMaskedLM.from_pretrained(exp_args.model_name_or_path)\n",
    "            \n",
    "            # Preprocess\n",
    "            train_dataset_prep, dev_dataset_prep, test_dataset_prep = prepare_dataset_for_prompt(\n",
    "                exp_args.task_name,\n",
    "                tokenizer,\n",
    "                train_subset,\n",
    "                dev_dataset,\n",
    "                test_dataset\n",
    "            )\n",
    "            \n",
    "            data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8 if torch.cuda.is_available() else None)\n",
    "            \n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=os.path.join(exp_args.output_dir, f\"{exp_args.task_name}_prompt_{len(train_subset)}\"),\n",
    "                learning_rate=exp_args.learning_rate,\n",
    "                per_device_train_batch_size=exp_args.train_batch_size,\n",
    "                per_device_eval_batch_size=exp_args.eval_batch_size,\n",
    "                num_train_epochs=exp_args.num_train_epochs,\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                save_strategy=\"epoch\",\n",
    "                logging_dir=os.path.join(exp_args.output_dir, \"logs\"),\n",
    "                logging_steps=50,\n",
    "                seed=exp_args.seed\n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset_prep,\n",
    "                eval_dataset=dev_dataset_prep,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator\n",
    "            )\n",
    "            \n",
    "            # Adjust steps vs epochs\n",
    "            step_per_epoch = len(train_dataset_prep) // training_args.per_device_train_batch_size\n",
    "            total_steps = step_per_epoch * training_args.num_train_epochs\n",
    "            if total_steps < exp_args.min_train_steps:\n",
    "                trainer.args.max_steps = exp_args.min_train_steps\n",
    "                trainer.args.num_train_epochs = 9999\n",
    "                trainer.args.evaluation_strategy=\"steps\"\n",
    "                trainer.args.save_strategy=\"steps\"\n",
    "            \n",
    "            trainer.train()\n",
    "            \n",
    "            dev_metrics = trainer.evaluate(eval_dataset=dev_dataset_prep)\n",
    "            test_metrics = trainer.evaluate(eval_dataset=test_dataset_prep)\n",
    "            \n",
    "            return dev_metrics, test_metrics\n",
    "        \n",
    "        for size in exp_args.subset_sizes:\n",
    "            if size > len(new_train):\n",
    "                continue\n",
    "            train_subset = new_train.select(range(size))\n",
    "            dev_metrics, test_metrics = train_and_evaluate_prompt(train_subset)\n",
    "            print(f\"[PROMPT] Subset={size}, Dev={dev_metrics}, Test={test_metrics}\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"approach must be either 'head' or 'prompt'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinfo2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
